# The Future of Data Systems

## 2. Unbundling Databases
### 3. Observing Derived State
journey of data: write path & read path
- portion of journey precomputed: write path:  eager evaluation
- portion of journey happening when someone asks: read path: lazy evaluation

Materialized views and caching
- shifting boundary between write-read path: do more work on the write path to save effor on the read path
  - `cache` / `materialized views`: precompute search results for a fixed set of the most common queries
  - uncommon queries served from the `index`

Stateful, offline-capable clients
- traditionally dominant client/server model: stateless client
- stateful client
  - "single-page" JavaScript web apps
    - client-side UI interaction
    - persistent local storage in the browser
  - mobile apps
    - no need for server for most UIs
    - store a lot of state on the device
- ‚û°Ô∏è `offline-first` applications
  - use local database on the same device
  - sync with remote servers in the background when connected to network
- on-device state = `cache` of state on the server

Pushing state changes to clients
- How to avoid stale cache on browsers?
- Recent protocols: EventSource API, WebSockets
  - keep an open TCP connection to the server ‚û°Ô∏è the server can actively push state changes to the browser(end user)
- = extending the write path all the way to the end user
- when end user offline: apply consumer offset!

End-to-end event streams
- stateful client tools
  - ... React! ...
  - manage internal client-side state by subscrbing to a stream of events representing user input or reponses from a server, structured similary to event sourcing
- extend it to the server-client: "real-time" architecture
- already possible: why not use all?
  - stateless & request/response: deeply ingrained in dominant tools
  - need to move from request/response to publish/subscribe

Reads are events too
- existing: reads are not events
  - treated as transient network requests going directly to the nodes
- stream processor can be treated as database
  - contain state needed for aggregations and joins
  - which can be queried by outside clients (reads!)
- new: represent both reads and writes as events and route them to the same stream operator
  - = perform stream-table join between the stream of read queires and the database
  - benefits
    1. ...???
    2. better tracking of causal dependencies
  - donwside: additional storage and I/O cost

Multi-partition data processing
- dataflow structure may not fit for single-partition queries
- dataflow structure enables distrbuted execution of complex queries combining data from *several partitions*
  - Storm: distrbuted RPC
    - Twitter: compute the number of users who have seen a URL = union of fllower sets of everyone who has tweeted the URL
  - fraud prevention
    - examine the purchasing user's reputation: IP address, email, addresses...
  - can provide an option for implementing large-scale applications

## 3. Aiming for Correctness
traditional approach: serializability & atomic commit
- only work on single datacenter :(

correctness in dataflow architecutres?

### 1. The End-to-End Argument for Databases

immutabilty is not enough

Hot Issue: Duplicate suppression
- `exactly-once`: arrange the computation so that the final effect is the same regardless of faults
- idempotence: one of the ways to achieve exactly-once
  - hard to implement
- How?
  1) Traditional appraoches
  - TCP: use packet sequnce number
    - only effective within the context of a single TCP connection :(
  - 2PC: no more 1:1 mapping between a TCP connection and a transaction
    - DB client - server: okay
    - end-user device - application server: not okay

  2) Uniquely identifying requests
  - How to maker request idempotent through several hops of netwrok communication? ‚û°Ô∏è `end-to-end` flow of the request
  - ex) generate a unique identifier for a request
    - requests table: duplicate suppresion + event log(‚Üí event sourcing)

The end-to-end arguement
- Many usages
  - duplicate suprresion
  - check the integrity of data
  - encryption
    - ex) TLS/SSL
- others are still useful

Applying end-to-end thinking in data systems
- Transactions are expensive
- Let's do it!

### 2. Enforcing Constraints
Will focus on uniqueness constraints: can be applied to other constraints
- Uniqueness constraint requires *consensus*

Database
- How to achieve consensus?
  - single-leader
  - uniqueness checking can be scaled out by partitioning
- multi-master replication ‚ùå

Uniqueness in log-based messaging
- log ‚û°Ô∏è `total order broadcast` ‚âà consensus
- unbundled database with log-based messaging üëç

Multi-partition request processing
- requesting multiple partitions
  - atomic commit: partitions cannot be processd indepndently :(
- Use partitioned logs üëç
  - Break down the multi-partition transaction into two differently partitioned stages and use the end-to-end request ID

### 3. Timeliness and Integrity

consistency = timeliness + integrity
- timeliness: users observe the system in an up-to-date state
  - if violated, inconsistency is temporary: retrying / waiting will fix it
- ‚≠êÔ∏èintegrity‚≠êÔ∏è: absence of corruption (no data loss, )
  - if violated, inconsistency is permanent: explicit checking and repair is needed
  - Atomicity & Durability from ACID required

Correctness of dataflow systems
- ACID transaction: timeliness and integrity cannot be decoupled
- event-based dataflow systems: timeliness and integrity can be decoupled
  - streaming: integrity better guaranteed

Loosely interpreted constraints
- consider the cost of constaint violation (cost of apology)

Coordination-avoiding data systems
- Summary
  1. Dataflow systems can maintain integrity guarantees on derived data without atomic commit, linearizabiliyt, or synchronous cross-partiton coordinataion
  2. Although strict uniqueness constraints require timeliness and coordination, many applications are actually fine with loose constraint violation as long as integrity is preserved throughout
- ‚û°Ô∏è dataflow systems can provide srvices w/o coordination while giving strong integry guarantees ‚û°Ô∏è better performance

### 4. Trust, but Verify

`auditing`: checking the integrity of data

event-based systems can provide better auditability
- being explicit about dataflow
- end-to-end argument üëç

auditable data system implementation
- cryptographic auditing
  - cryptocurrencies, blockchains, distributed ledger technologies ...
  - use cryptographic tools to prove the integraity of a system robust to a wide range f hardware and software issues
