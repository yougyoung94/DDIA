# Partitioning
Scalability ‚≠êÔ∏è: *Spread the data and query load evenly across multiple machines*

## 1. Partitioning and Replication
- Usually used in combination
- Strategies are mostly independent of each other
- Single-leader replication + partitioning

    <img src="/assets/images/figure_6_1.png" alt="figure_6_1.png" style="height: 100px; width:100px;"/>

## 2. Partitioning of Key-Value Data
How to prevent `skewedness`? How to avoid `hot spot`?

### 1. Partitioning by Key Range
How
- keys are *sorted* in each partition 
- assign a continuous range of keys to each partition

Good
- efficient range queries
- can be treated as [concatenated index](../part1/03.md#5\)-Other-Indexing-Structures)
    - ex) `year-month-day` as key: easily fetch data from a particular month

Bad
- risk of hot spots if the applications often accesses keys close together in the sorted order  
    - ex) Partitioned by date: all the writes of the day go to corresponding partition

Used in
- BigTable, HBase, RethinkDB, MongoDB < v.2.4

Common rebalancing strategy
- dynamic partitioning

### 2. Partitioning by Hash of Key
How
- use hash function for keys
    - no need to be cryptographically strong
    - language built-in hash functions may not be suitable
- assign a range of *hashes* to each partition
    - partition boundaries may be evenly spaced
    - or chosen pseudorandomly: `consistent hashing`(=`hash mod N`)
    
Good
- more evenly distributed keys and load

Bad
- inefficient range queries
    - MongoDB: any range query sent to all the partitions
    - Riak, Couchbase, Voldemort: range queries on primary key not supported

Used in
- MongoDB, Riak, Couchbase, Voldemort

Common rebalancing strategy
- fixed partitioning
- MongoDB: dynamic partitioning

### 3. Compromise
Cassandra `compound primary key`

How
- Use one part of the key to hash ‚û°Ô∏è partition
- Use another part as concatenated index to sort within the partition

Good for one-to-many relationships
- ex) user-to-posts

### 4. Skewed Workloads and Relieving Hot Spots
reducing skew: no automatic solution ‚û°Ô∏è the application's responsibility
- ex) a very `hot` key: add random number to the key
    - üëç writes to the key will be split evenly across partitions
    - üëé reads have to read all data from all the keys and combine it
    - üëé requires additional bookkeeping
        - track the ***few*** `hot` keys being split

## 3. Partitioning and Secondary Indexes
secondary index: recap [Other Indexing Structures](../part1/03.md#5\)-Other-Indexing-Structures)
- search servers ‚ù§Ô∏è: ElasticSearch, Solr
- don't map neatly to partitions ü§Æ

### 1. Local Index: Partitioning Secondary Indexes by Document
How
- index covers data in ***each*** partition
- index partitioned the ***same*** as the primary key index

Good
- efficient write
    - only a single partitions needs to be updated on write

Bad
- expensive read on secondary indexes: `scatter/gather`
    - send the read query to *all* partitions and combine the results 

Used in
- MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud, VoltDB

secondary index queries served from a single partition, but that is not alawys possible especially when you're using multiple secondary indexes in a single query
: Ïñ¥Ï∞®Ìîº ÏïàÎêòÎäî Í±∞ ÏïÑÎãåÍ∞ÄÏöî...???;;;

### 2. Global Index: Partitioning Secondary Indexes by Term
How
- index covers data in ***all*** partitions
- index partitioned ***differently*** from the primary key index
    - hash / range (= primary key)

Good
- more efficient reads

Bad: a write to a single document may require a distributed transaction across multiple partitions 
- writes are slower and more complicated
- hard to keep indexes updated: often asynchronously updated

Used in
- Amazon DynamoDB, Riak

## 4. Rebalancing Partitions
Moving load from one node in the cluster to another

Needs to
1. load should be balanced after rebalancing
2. should not interrupt read and writes
3. should be fast and minimize the network and disk I/O load

### 1. Strategies for Rebalancing
#### 0. hash mod N ‚ùå
HUGE rebalancing if the number of nodes N changes ‚û°Ô∏è violates #3

#### 1. Fixed number of partitions
How
- create a fixed number of partitions and assign them to each node
- move *entire* partitions from one node to another when node is added or removed

Used in
- Riak, Elasticsearch, Coucnbase, Voldemort

Good
- operationally simple

Bad
- Choosing the right number of partitions is hard if data size is highly variable
    - too small: insufficient nodes to store data
    - too large: counterproductive

#### 2. Dynamic partitioning
How
- split the partition when data grow / merge partitions when data decrease

Used in
- especially when using key range partitioning: HBase, RethinkDB 
- hash partitioning: MongoDB >= v.2.4

Good
- adaptive number of partitions

Bad
- cold start: single partition ‚û°Ô∏è allow pre-splitting

#### 3. Partitioning proportionally to nodes 
dynamic partitioning: # partitions ‚àù data size
fixed partitioning: partition size ‚àù data size

How
- Have a fixed number of partitions per node ‚û°Ô∏è # partitions ‚àù # nodes
- when node is added: randomly choose the fixed number of partitions to split, take the half 

Used in
- for hash partitioning
- Cassandra, Ketama

Good
- partition size ‚àù number of nodes ‚àù data size
- keeps the size of each partition fairly stable like dynamic partitioning

### 2. Operations: Automatic or Manual Rebalancing
Manual!!
- automatic rebalancing: convenient but unpredictable
    - especially when used with automatic failure detection

## 5. Request Routing
`service discovery` problem
- client making a request: which node to connect to?

<img src="/assets/images/figure_6_7.png" alt="figure_6_7.png" style="height: 100px; width:100px;"/>

1. contact any node: the contacted node will forward the request to the appropriate node, *and receive and pass* reply to the client
2. contact routing tier first: forward request to the node
3. client itself is aware of the partitions: connect directly to th enode

How to update `the knowledge of which partition is assigend to which node`?
- separate coordination system (2, 3)
    - ZooKeeper ‚≠êÔ∏è‚≠êÔ∏è
      
        <img src="/assets/images/figure_6_7.png" alt="figure_6_8.png" style="height: 100px; width:100px;"/>
      
        1. each node registers itself to ZooKeeper
        2. ZooKeeper maintains the authoritative mapping of partitions to nodes
        3. routing tier / client subscribes to the metadata in ZooKeeper: notified when changes
        - HBase, SolrCloud, Kafka, Espresso Helix
    - MongoDB: similar architecture but has its own `config server` and `mongos` daemons as the routing tier
- gossip protocol (1)
    - no external dependency but more complexity
    - Cassandra, Riak
- NO Update (no rebalance)
    - Couchbase (2)

Clients can use DNS to find the IP addresses

### Parallel Query Execution
massively parallel processing (MPP) query optimizer
- for more complex queries including join, filtering grouping and aggregation operations
- ‚ë† *break* the query into execution stages and partitions which can be executed in ‚ë° *parallel*
