# Storage and Retrieval

- Why learn this?: to select appropriate storage engine
- Topics
    1. Storage engines: log-structured & page-oriented
    2. Transaction Processing vs. Analytics
    3. Storage engines: optimized for analytics


## 1. Data Structures That Power Your Databases
Simplest database: `db_set` & `db_get`
- `db_set`: simply append to a file ➡ good performance
    - `log`: ***append-only, immutable*** data file
        - ❗merging & compaction❗ required
            - `compaction`: throwing away duplicate keys in the log, and keeping only the most recent update for each key
- `db_get`: scan the entire DB file ➡ O(n) terrible performance 
    - `index`: ***additional*** structure derived from the primary data
        - ❗️tradeoff❗: speed up read queries ↔ incur overhead on writes
        - `key-value index`(≈`primary key`): ***uniquely*** identifies one row/document/vertex
            1. Hash Index (log-structured)
            2. LSM-Trees (log-structured)
            3. B-Trees (page-oriented)
        - `secondary index`: the indexed values are not unique


### 1) Hash Indexes
Examples
- storage engines: Bitcask

Index: Hash map
- Keep an ❗️in-memory❗️ hash map where ***every key*** is mapped to a byte offset in the data file

Good for
- the value for each key is **updated** frequently
- i.e. large number of writes per key / all the keys fit in memory

Segment file format: `log` ➡ ***append-only***
- Each segment has its own in-memory hash map
- check from the latest's hash map

merging and compaction
- can be done in a background thread
    - read → old segment / write → latest segment file
- once it's complete → switch read requests to the new merged segment & delete the old ones
    - keeps the number of segments small ➡ not many hash maps to check

issues in implementation
- File format: binary format (length in bytes + raw string)
- Deleting records: `tombstone`
- Crash recovery: if DB crashes, in-memory hash maps are lost
    - Bitcask: store snapshot of each hash map on disk
- Partially written records: DB crash while appending record to the log
    - Bitcask: checksum
- Concurrency control: writes are appended to the log in a strictly sequential order
    - write: only one thread
    - read: multiple threads

👍 append-only log
- Appending & segment merging are *sequential* write operations
    - much faster than *random* writes
    - sequential vs. random writes
        - write without intermediate seeks vs. a pattern of seek-write-seek-write...
- concurrency and crash recovery are simpler
- merging ➡ avoids data fragmentation

👎 limitations → no more in the next section
- The hash maps must fit in memory
    - on-disk hash map 👎
        - require a lot of random access I/O
        - expensive to grow when it's full 
        - hash collisions 
- Inefficient range queries

---
- Side Notes
    - hash map vs. hash table? (다르대요... 근데... 노이해... Help)

### 2) LSM-Trees(Log-Structured Merge-Tree)
Examples
- storage engine library: LevelDB, RocksDB 
- used in: Cassandra, HBase
- indexing engine: Lucene (used by Elasticsearch and Solr)
    - `term dictionary`: mapping from term to postings list is kept in SSTable-like sorted file (merged in the background as needed)

Segment file format: SSTable(Sorted String Table)
- Each segment has its own sparse index
- key-value pairs are sorted by key
- unique key

Index: Sparse index in memory
- 👍👍👍 than log segments with hash indexes
    1. no longer need to keep an index of all the keys in memory
    2. Sparse index
       
        ➡ read requests are all range queries
       
        ➡ group records into a block and compress it before writing it to disk. each entry of sparse index then points at the start of a compressed block.
       
        ➡ save disk space & reduce I/O bandwidth use

Merging & Compaction: MergeSort
- 👍👍👍 than log segments with hash indexes
    - Merging segments is simple and efficient even if the files are bigger than the available memory (mergesort)
- assume that we always merge adjacent segments

Constructing and maintaining SSTables: How to sort by key??
- write
    1. write → `memtable`: *in-memory* balanced tree data structure
    2. when memtable gets bigger → write it out *to disk* as an SSTable file
-  read
    1. find the key in the memtable
    2. read on-disk segments by latest
- merge and compact from time to time
- DB crash ➡ memtable lost 😱
    - keep a separate append-only log on disk
    - discarded every time memtable written out to an SSTable

Issues in implementation: Performance optimizations
- slow when looking up keys that do not exist in the DB: check memtable, all SSTables ➡ `Bloom filters`
    - `Bloom filters`: data structure. tells if a key does not appear in the DB. 
- How to determine the order & timing of merging & compaction
    - size-tiered: HBase, Cassandra
        - newer and smaller SSTables successively merged into older and larger SSTables
    - level-tiered: LevelDB, RocksDB, Cassandra
        - key range is split up into smaller SSTables and older data is moved into separate "levels"
        - allows compaction to proceed more incrementally and use less disk space 

Strengths
- Sparse index ➡ works well even when the dataset is much bigger than the available memory
- Data stored in sorted order ➡ efficient range queries
- Sequential writes ➡ high write throughput

### 3) B-Trees
The most common type of index

Index: Pointer on disk

log-structured vs. page-oriented indexes
- log-structured
    - break the database down into *variable-size* `segments`
    - write a segment *sequentially*
- page-oriented
    - break the database down into *fixed-size* `pages`(`blocks`): hardware-friendly
        - each page is identified by an address or location ➡ allow reference
        - `branching factor`: number of references to child pages
    - read or write one page at a time

update, add, delete: Keep it ***balanced*** (n keys always has depth of O(log n))
- update: find the leaf page and update
- add: if leaf page full → split leaf page into half-full pages & update parent page
- delete: ㅎㅡㅎ

Making B-Trees reliable
- write: *overwrite* (↔ log-structured indexes) a page on disk with new data
    - all references to that page remain intact
    - an actual hardware operation
        - magnetic hard drive
            1. move disk head to the right place
            2. wait for the right position on the spinning platter to come around 
            3. overwrite the appropriate sector with new data
        - SSD: more complicated (must erase and rewrite fairly large blocks of a storage chip a time)
- some require overwriting several different pages ➡ may crash before done
    - `write-ahead log`: append-only file on disk. written before applied
- concurrency control: updating pages complicated
    - `latches` (lightweight locks)

B-Tree optimizations
- copy-on-write scheme: crash recovery & concurrency control 
    - A modified page is written to a different location, and a new version of the parent pages in the tree is created
    - examples
        - Snowflake: Clone
        - Docker: If a file or directory exists in a lower layer within the image, and another layer (including the writable layer) needs read access to it, it just uses the existing file. The first time another layer needs to modify the file (when building the image or running the container), the file is copied into that layer and modified. This minimizes I/O and the size of each of the subsequent layers.
- B+ Tree: abbreviate keys ➡ save space in pages
- pages can be positioned anywhere on disk ➡ Scanning over a large part of the key range in sorted order may be inefficient
- Additional pointers
    - ex. each leaf page reference to its siblings

### 4) B-Trees vs. LSM-Trees
Rule of thumb
- faster writes: LSM-Trees
- faster reads: B-Trees
    - LSM-Trees: have to check several different structures and SSTables at different stages of compaction

👍 LSM-Trees
- `write amplification`: one write to the DB resulting in multiple writes to the disk over the course of the DB's lifetime
    - direct performance cost in write-heavy applications
        - the more writes to disk, the fewer writes per second it can handle within the available disk bandwidth
1. Lower write amplification
    - Sustain high throughput than B-Tree
    - sequentially write compact SSTable files vs. overwrite several pages
        - magnetic hard drives: sequential writes are much faster than random writes
2. Lower storage overhead & reduced fragmentation
    - B-Tree: leave disk space unused due to fragmentation 
    - LSM-Tree: can be compressed better, produce smaller files on disk
        - not page-oriented 
        - merging & compaction to remove fragmentation(especially, level-tiered 👍)
- SSD
    - the impact of storage engine's write pattern is less pronounced
        - it internally uses log-structured algorithm to turn random writes into sequential writes on the underlying storage chips
    - still, representing data more compactly allows more read and write requests within the available I/O bandwidth 

👍 B-Trees
- Downsides of LSM-Trees
    1. compaction process can sometimes interfere with the performance of ongoing reads and writes
        - disks have limited resources
        - high `tail latencies`: B-Trees can be more predictable
    2. compaction may not keep up with the rate of incoming writes
        - In case of high write throughput + careless compaction configuration
            - The number of unmerged segments increase (may run out of disk space)
            - reads slow down (need to check more segment files)
        - SSTable-based storage engines do not throttle the rate of incoming writes ➡ explicit monitoring needed
- Each key exists in ***exactly one place*** in the index
    1. No compaction needed
    2. offer strong transactional semantics
        - transaction isolation using lock: locks can be directly attached to B-Tree index 

### 5) Other Indexing Structures
`secondary index` vs. `key-value index`
- the indexed values are not unique vs. unique

Storing values within the index
- index
    - key: the thing queries serach for
    - value can be EITHER
        - actual row/document/vertex → `clustered index` 
            - 👍: speed up reads
            - 👎: add overhead on writes & more effort to enforce transactional guarantees
        - OR reference to the row → `nonclustered index`
            - `heap file`: the place where rows are stored
- `nonclustered index`
    - avoids duplication when multiple secondary indexes are present
    - updating
        - overwritten (new value not larger than the old value)
        - moved to a new location
            - update all indexes
            - OR leave forwarding pointer in the old location
- `clustered index`
    - hop from the index to the heap file is too much of a performance penalty for reads
    - store the indexed row directly within an index
- `covering index`
    - stores some of a table's columns within the index

Multi-column indexes: query multiple columns simultaneously
- `concatenated index`
    - combine several fields *in defined order* into one key by appending one column to another
    - useless if you want to query subsets
- `multi-dimensional index`
    - more general way
    - commonly used in geospatial data
    - mix up B-Tree, R-Trees ...

Full-text search and fuzzy indexes
- look for *similar* keys: *fuzzy* querying

Keeping everything in memory
- log, SSTable, page...: all due to limitations of disks
    - Why disk?
        1. more durable
        2. lower cost per gigabyte
    - Now RAM getting cheaper...!
- `in-memory database`
    - keep datasets entirely in memory (maybe distributed among several machines)
    - durability issue: when DB restarts
        - if caching use only then data lost OK (ex. Memcached)
        - many methods: special hardware, writing log of changes to disk, writing periodic snapshots to disk, replicating the in-memory state to other machines ...
    - Databases
        - VoltDB, MemSQL, Oracle TimesTen
        - RAMCloud
        - Redis, Couchbase: weak durability by writing to disk asynchronously
    - Performance advantage b/c
        - faster reads (X)
            - disk-based storage engine may never read from disk: the OS caches recent disk blocks in memory
        - faster writes (O)
            - avoid the overheads of encoding in-memory data structures in a format that can be written to disk
    - memory!
        - provide data models difficult to implement with disk-based indexes
    - could be extended to support datasets larger than the available memory w/o disk-centric architecture
        - `anti-caching`
            - evict the least recently used data from memory to disk when there is not enough memory, loading it back into memory when it is accessed again
            - similar to virtual memory and swap files
            - manage memory more efficiently than OS: work at the granularity of individual records rather than entire memory pages
            - still requires indexes to fit entirely in memory
            