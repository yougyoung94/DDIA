# Encoding and Evolution

Changes in applications
- schema changes 
- ‚û°Ô∏è ***code changes***

How to deal with?
- schema changes: Database `schema-on-write` vs. `schema-on-read`
- `compatibility`: old and new versions of code, old and new data formats coexist at the same time
    - codes cannot change at once becuase
        - rolling upgrade
        - client may not update
    - which code reads data??
    - `backward compatibility`: *newer* code reads data written by *older* code
    - `forward compatibility`: *older* code reads data written by *newer* code
        - tricky: older code should ignore additions made by a newer version of code

Topics
1. formats for encoding data
    - how they handle schema changes
    - how they support compatibility
2. how those formats are used for data storage and web communications

## 1. Formats for Encoding Data

`encoding` & `decoding`
- data in 2 different representations
    1. in-memory representation
        - when?: data in memory
        - data is kept in objects, structs, lists, arrays, hash tables, trees, etc.
        - optimized for efficient access and manipulation by the CPU (typically using *pointers*)
    2. byte sequence
        - when?: write data to a file / send over the network
        - self-contained
        - no pointer ‚û°Ô∏è different from in-memory
- `encoding`: translation from *in-memory* representation to a byte sequence
- `decoding`: the reverse

### 1) Language-Specific Formats
Not recommended

üëç
- allow in-memory objects to be saved and restored with minimal additional code

üëé
- ‚≠êÔ∏è tied to a particualar programming language
- ‚≠êÔ∏è neglect versioning ‚û°Ô∏è tough compatibility
- security issue
    - decoding process needs to instantiate arbitrary classes
    - decode an arbitrary byte sequence ‚û°Ô∏è instantiate arbitrary class
- neglect efficiency
    - CPU time taken to encode or decode
    - size of the enocded structure

### 2) JSON, XML, and Binary Variants: Schemaless
standardized encodings

`Textual Formats` 
- human-readable
- remain popular
    - JSON, XML, CSV
- problems: schema üò£
    - ambiguity around the encoding of numbers
        - XML, CSV: not distinguish between a number and a string consisting of digits
        - JSON: not distinguish integers and floating-point numbers (large number 2^53 üò±)
    - JSON, XML: don't support binary strings
        - binary string: sequence of bytes without a character encoding
        - workaround: tricky and increase data size
    - Weak schema support
        - JSON, XML: complicated
        - CSV: No schema

`Binary Encoding` variant
- for JSON, XML
- not widely adopted
- üëç: extend the set of datatypes
- ü§î: *small space reduction*
    - No prescribed schema ‚û°Ô∏è include *all* field names
    - Íµ≥Ïù¥...?

### 3) Thrift and Protocol Buffers: Schema
`Binary Encoding` libraries

characteristics 
- require `schema`
    - code generation tool: takes schema definition and prodcues class implementing the schema
    - required fields: runtime check
- encoded record: concatenation of its encoded fields
    - each field identified by *tag number*

Types
1. Thrift: Binary Protocol
    - Facebook
    - no field name: ***field tags***
2. Thrift: Compact Protocol
    - more compact
        - packing the field type and field tag into a single byte
        - use variable-length integers
3. Protocol Buffers
    - Google
    - mostly similar to compact protocol

schema evolution
1. CANNOT CHANGE a field's tag
2. adding a new field
    - `backward`: a new field should be either OPTIONAL or have a DEFAULT VALUE
    - `forward`: older code ignore new fields
3. removing a field: a required field can never be removed
    - `backward`: ignore old field
    - `forward`: a removed field is OPTIONAL and its tag number never to be reused
4. Datatypes
    - risky: Check if they are convertible
        - ex) 32-bit int in old code vs. 64-bit int in new code ‚û°Ô∏è tuncation
    - list of values
        - Protocol Buffers: No list or array datatype: `repeated` 
            - OPTIONAL datatypes ‚ÜîÔ∏è repeated types
        - Thrift: List datatype
            - No evolution
            - but support nested lists

### 4) Avro: Schema
characteristics
- Subproject of Hadoop
- use schema: `reader's schema` & `writer's schema`
    - `reader's schema`: used in decoding
    - `writer's schema`: used in encoding
        - large file with lots of records
            - included at the beginning of the file
        - Database with individually written records
            - include a version number at the beginning of every encoded record
            - keep a list of schema versions in database
            - ex) Espresso
        - Sending records over a network connection
            - negotiate the schema version on connection setup and then use that schema for the lifetime of the connection
- no `required/optional`: `union` & default
- encoded record: values concatenated together (no datatypes or field tags) ‚û°Ô∏è the most compact
    - use variable-length integers

Schema evolution
- achieved by schema resolution: compare the writer's schema and reader's schema side by side and translate the data from the former to the latter
1. add or remove a field with default values (= Thrift & Protocol Buffers)
    - `null` should be included in `union` branches (as the first if it is the default value)
2. changing the datatypes
    - if convertible: OK
    - adding a branch to a union: backward O / forward X
3. changing the name of a field: backward O / forward X

Advantages
- no tag number ‚û°Ô∏è friendlier to dynamically generated schemas
    - ex) Using a RDB
        - schema changes: generate a new Avro schema from the updated DB schema and export data in the new Avro schema
        - Thrift and Protobuf: the field tags have to be assigne by hand
- can be used with / without code generation
    - statically typed programming languages: supported but not required
        - only needs writer's schema
    - dynamically typed programming languages & dynamically generated schema: unnecessary / obstacle

### 5) The Merits of Schema
merits of schema
1. more compact
2. easier and more valuable documentation
    - schema is required for decoding ‚û°Ô∏è sure it is up to date
3. easier to check forward and backward compatibility of schema changes
4. Statically typed programming languages: 

schema evolution ‚û°Ô∏è
  - flexibility like schemaless JSON databases
  - `+` better guarantees about your data 
  - `+` better tooling

## 2. Modes of Dataflow

How data flow from *one process* to *another* not sharing memory?
- *Who* encodes the data and *who* decodes it?
1. Via databases
2. Via service calls
3. Via asynchronous message passing

### 1) Dataflow Through Databases
be aware of data loss due to version difference
    - several different processes accessing a database at the same time
    - newer code encodes data ‚Üí older code decodes & *reencodes* the data: added field might be lost

dealing with schema changes: SCHEMA EVOLUTION
- migrating(rewriting) data into a new schema: expensive! should be avoided 
    - RDB: allow simple schema changes (e.g. adding a new column with a null default value) w/o rewriting existing data
        - brave MySQL: Rewrite everything ü§Æ
    - Espresso (Linkedin, Document DB): uses Avro ‚û°Ô∏è use Avro's schema evolution rules

Archival storage
- data dump is encoded with the latest schema: written in one go & ***immutable***
- ex) Avro, Parquet(analytics-friendly column-oriented)

### 2) Dataflow Through Services: REST and RPC
communicate over network: `client` & `server`
- server exposes API (i.e. `service`)
    - API (source: https://aws.amazon.com/what-is/api/)
        - mechanisms that enable two software components to communicate with each other using a set of definitions and protocols
        - vs. Database
            - =
                - allow clients to submit and query data
            - !=
                - services can impose fine-grained restrictions on what clients can and cannot do: provides a degree of `encapsulation` 
        - `client` = application sending the request / `server` = application sending the response
        - types
            1. SOAP APIs
                - Simple Object Access Protocol
                - exchange messages using XML
            2. RPC APIs
                - Remote Procedure Calls
                - the client executes a function on the server & the server sends the output back to the client 
            3. Websocket APIs
                - use JSON objects to pass data
                - two-way communication: the server send call back messages to connected clients ‚û°Ô∏è more efficient than REST API
            4. REST APIs
                - Representational State Transfer: `GET` `PUT` `DELETE` etc
                    - ***statelessness***: server do not save client data between requests
                - the client sends requests *with input* to the server as data
                - the server uses client input to execute internal functions & returns ***plain data*** output back to the client
- client requests to the API
    - types
        - web browsers
        - `Ajax`: allows web pages to be updated ***asynchronously*** by exchanging data with a web server behind the scenes. This means that it is possible to update parts of a web page, without reloading the whole page.
        - etc.

microservices architecture
- a server itself can be a client to another service
- make services independently deployable: decompose a large application into similar services by area of functionality
- ‚≠êÔ∏è Maintainability: operable, simple, evolvable

Web services: SOAP, REST API

RPC API
### 3) Message-Passing Dataflow
